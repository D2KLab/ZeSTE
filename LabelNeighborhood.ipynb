{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import pickle\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import multiprocessing as mp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show that when using just Numberbatch you get worse results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import get_word_neighborhood, filter_neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_cach_path = '/data/zeste_cache/neighborhoods/'\n",
    "args_prefetch_path = '/data/zeste_cache/prefetch_neighborhoods/'\n",
    "args_numberbatch_pickle = '/home/semantic/harrando/zeste/numberbatch-en-19.08-en.pickle'\n",
    "args_dataset_csv_path = 'datasets/bbc_dataset.csv'\n",
    "args_labels_mapping = '20ng_labels_mapping.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberbatch = pickle.load(open(args_numberbatch_pickle, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_sim = numberbatch.similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['business', 'entertainment', 'politics', 'sport', 'tech']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset = pd.read_csv(args_dataset_csv_path)\n",
    "raw_corpus = df_dataset.text.tolist()\n",
    "gt_labels = df_dataset.label.tolist()\n",
    "unique_labels = sorted(set(gt_labels))\n",
    "\n",
    "unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=2, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_counter = TfidfVectorizer(ngram_range=(1,3), min_df=2)\n",
    "ngram_counter.fit([' '.join(preprocess(d)) for d in raw_corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vice versa',\n",
       " 'business world',\n",
       " 'past present',\n",
       " 'video game',\n",
       " 'also used',\n",
       " 'digital technology',\n",
       " 'take photo',\n",
       " 'last year',\n",
       " 'picture message',\n",
       " 'big business']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_mapping = dict(l.strip().split('\\t') for l in open(args_labels_mapping))\n",
    "labels = sorted(set(labels_mapping.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['baseball',\n",
       " 'car',\n",
       " 'cryptography',\n",
       " 'electronics',\n",
       " 'graphic',\n",
       " 'gun',\n",
       " 'hardware',\n",
       " 'hockey',\n",
       " 'medicine',\n",
       " 'middle_east',\n",
       " 'motorcycle',\n",
       " 'politics',\n",
       " 'religion',\n",
       " 'sale',\n",
       " 'space',\n",
       " 'windows']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_neighborhood(label, depth=1, numberbatch=numberbatch, cache_path=args_cach_path, prefetch_path=args_prefetch_path):\n",
    "    # In case the requested label does not appear in the cache\n",
    "    \n",
    "    pickle_path = os.path.join(args_cach_path, label+'.pickle')\n",
    "    if depth == 0 or not os.path.exists(pickle_path) or label not in numberbatch:\n",
    "        return {}\n",
    "    \n",
    "    # if already computed\n",
    "    prefetch_path = os.path.join(prefetch_path, str(depth) + '/' + label+'.pickle')\n",
    "    if depth > 1 and os.path.exists(prefetch_path):\n",
    "        # print('Prefetching the pickle for label', label, '..')\n",
    "        return pickle.load(open(prefetch_path, 'rb'))\n",
    "\n",
    "    # Get immediate label neighborhood\n",
    "    similarities = ['simple', 'compound', 'depth', 'harmonized']\n",
    "    neighborhood = pickle.load(open(pickle_path, 'rb'))\n",
    "    for node in neighborhood:\n",
    "        # we add the possiblity of defining multiple similarity methods for nodes that are not directly connected\n",
    "        # to the Label node\n",
    "        neighborhood[node]['rels'] = [tuple(neighborhood[node]['rels'])]\n",
    "        neighborhood[node]['sim'] = {sim:neighborhood[node]['sim'] for sim in similarities}\n",
    "    \n",
    "    # Connect to n-hops labels\n",
    "    hops = 1\n",
    "    to_visit_next = list(neighborhood.keys())\n",
    "    while hops < depth:\n",
    "        next_hop = []\n",
    "        while len(to_visit_next) > 0:\n",
    "            current_node = to_visit_next.pop()\n",
    "            if current_node in stopwords.words('english'):\n",
    "                continue\n",
    "            cnn = get_word_neighborhood(current_node, depth=1)\n",
    "            for word in cnn:\n",
    "                if word not in neighborhood:\n",
    "                    neighborhood[word] = {'from':[], 'rels': [], 'sim':{}}\n",
    "                    sim_dict = {sim: 0.0 for sim in similarities}\n",
    "                else:\n",
    "                    sim_dict = neighborhood[word]['sim']\n",
    "                    \n",
    "                neighborhood[word]['from'].append(current_node)\n",
    "                neighborhood[word]['rels'].append(tuple(cnn[word]['rels']))\n",
    "                if word in numberbatch:\n",
    "                    sim_label_word = numberbatch.similarity(label, word)\n",
    "                    sim_dict['simple'] = max(sim_dict['simple'], sim_label_word)\n",
    "                    sim_dict['depth']  = max(sim_dict['depth'], sim_label_word / (hops + 1))\n",
    "                    if current_node in numberbatch:\n",
    "                        sim_cn_word = numberbatch.similarity(current_node, word)\n",
    "                        sim_label_cn = numberbatch.similarity(label, current_node)\n",
    "                        compound = sim_label_cn * sim_cn_word\n",
    "                        harmonized = 2*compound / (sim_label_cn + sim_cn_word)\n",
    "                        sim_dict['compound'] = max(sim_dict['compound'], compound)\n",
    "                        sim_dict['harmonized'] = max(sim_dict['harmonized'], harmonized)\n",
    "                    else:\n",
    "                        sim_dict['compound'] = max(sim_dict['compound'], sim_label_word)\n",
    "                        sim_dict['harmonized'] = max(sim_dict['harmonized'], sim_label_word)\n",
    "            \n",
    "                # print('From label' + label + '- current node:' + current_node + ', word:' + word)\n",
    "                neighborhood[word]['sim'] = sim_dict\n",
    "                next_hop.append(word)\n",
    "        \n",
    "        hops += 1\n",
    "        to_visit_next = next_hop\n",
    "        \n",
    "    # save \n",
    "    if depth > 1:\n",
    "        pickle.dump(neighborhood, open(prefetch_path, 'wb'))\n",
    "\n",
    "    return neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_neighborhood(label_words, depth=2):\n",
    "\n",
    "    ns = []\n",
    "    words = label_words.split(';')\n",
    "    for word in words:\n",
    "        ns.append(get_word_neighborhood(word, depth=depth))\n",
    "    neighborhood = ns[0].copy()\n",
    "    \n",
    "    for current_node, cnn in zip(words[1:], ns[1:]):\n",
    "        for word in cnn:\n",
    "            if word in neighborhood:\n",
    "                neighborhood[word]['from'].append(current_node)\n",
    "                neighborhood[word]['rels'].append(tuple(cnn[word]['rels']))\n",
    "                neighborhood[word]['sim'] = {s:max(neighborhood[word]['sim'][s], cnn[word]['sim'][s]) for s in cnn[word]['sim']}\n",
    "            else:\n",
    "                neighborhood[word] = {}\n",
    "                neighborhood[word]['from'] = [current_node]\n",
    "                neighborhood[word]['rels'] = [tuple(cnn[word]['rels'])]\n",
    "                neighborhood[word]['sim']  = cnn[word]['sim'].copy()\n",
    "\n",
    "    return neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(document):\n",
    "    document = document.replace(\"'ll\", ' will').replace(\"s' \", 's').replace(\"'s\", '').replace(\"-\", '_')\n",
    "    document = ''.join(c for c in document if c not in '!\"#$%&\\'()*+,./:;<=>?@[\\\\]^`{|}~')\n",
    "    document = [w for w in document.lower().split(' ') if w not in stopwords.words('english')]\n",
    "    document = [lemmatizer.lemmatize(w) for w in document if w != '']\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_neighborhoood(neighborhood_original, allowed_rels, sim, keep):\n",
    "    if allowed_rels == 'all' and sim == 'simple' and keep == 'all':\n",
    "        return neighborhood_original\n",
    "\n",
    "    neighborhood = copy.deepcopy(neighborhood_original)\n",
    "    \n",
    "    if allowed_rels == 'related':\n",
    "        allowed_rels = ['DefinedAs', 'DerivedFrom', 'HasA', 'InstanceOf', 'IsA', 'PartOf', 'RelatedTo', 'SimilarTo', 'Synonym', 'Antonym']\n",
    "    \n",
    "    if allowed_rels != 'all':\n",
    "        nodes = list(neighborhood.keys())\n",
    "        for node in nodes:\n",
    "            if not any(rel in rels for rel in allowed_rels for rels in neighborhood[node]['rels']):\n",
    "                del neighborhood[node]\n",
    "                continue\n",
    "    \n",
    "    if keep != 'all':\n",
    "        all_scores = sorted([neighborhood[node]['sim'][sim] for node in neighborhood], reverse=True)\n",
    "        if keep.startswith('top') and keep.endswith('%'):\n",
    "            keep = int(keep[3:-1]) / 100.\n",
    "            cutoff_score = all_scores[int(keep*(len(all_scores)))-1]\n",
    "        elif keep.startswith('top'):\n",
    "            keep = int(keep[3:])\n",
    "            cutoff_score = all_scores[min(len(all_scores)-1,keep)]\n",
    "        elif keep.startswith('thresh'):\n",
    "            cutoff_score = float(keep[6:])\n",
    "        nodes = list(neighborhood.keys())\n",
    "        for node in nodes:\n",
    "            node_sim = neighborhood[node]['sim'][sim]\n",
    "            if node_sim <= cutoff_score:\n",
    "                del neighborhood[node]\n",
    "                continue\n",
    "\n",
    "    return neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_2 = labels_at_depth[2]['space']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "gun_2 = labels_at_depth[2]['gun']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "lns2 = {'space': space_2, 'gun': gun_2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'simple': 0.75864804,\n",
       " 'compound': 0.7698828,\n",
       " 'depth': 0.3793240189552307,\n",
       " 'harmonized': 0.870072858764631}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tech_2['hierological']['sim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = filter_neighborhoood(tech_2, 'all', 'simple', 'top100')\n",
    "len(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For depth  2 :\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "039b02f4490a49e19d6905375ed699d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=17.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For depth  3 :\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c3d6d11435243c68a96a8891926e46e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=17.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "labels_at_depth = {}\n",
    "\n",
    "for depth in [2, 3]:\n",
    "    print('For depth ', depth, ':')\n",
    "    labels_at_depth[depth] = {}\n",
    "    for label in tqdm(labels): \n",
    "        labels_at_depth[depth][label] = get_label_neighborhood(label, depth)import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(tokens, label_neighborhood, sim, ngrams, normalize):   \n",
    "    if ngrams:\n",
    "        doc = ' '.join(tokens)\n",
    "        for ngram in ngrams:\n",
    "            if ngram in doc:\n",
    "                tokens.append(ngram)\n",
    "    \n",
    "    score = 0\n",
    "    inter = 0\n",
    "    for token in tokens:\n",
    "        if token in label_neighborhood:\n",
    "            score += label_neighborhood[token]['sim'][sim]\n",
    "            inter += 1 \n",
    "            \n",
    "    if normalize == 'inter_len':\n",
    "        return round(score / max(inter, 1), 6)\n",
    "    elif normalize == 'max_score':\n",
    "        return round(score / sum([label_neighborhood[node]['sim'][sim] for node in label_neighborhood]), 6)\n",
    "    else:\n",
    "        return round(score, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.253939"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(preprocess('I love some space video game technology!'), tech_2, 'simple', ngrams=ngrams, normalize='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [preprocess(d) for d in ['I love science', 'I love weapons', 'I love time travel', 'I love war']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_dataset(docs, sorted_labels, labels_neighborhoods, sim, ngrams, normalize):\n",
    "    scores = np.zeros((len(docs), len(sorted_labels)))\n",
    "    for i, doc in enumerate(docs):\n",
    "        for j, label in enumerate(sorted_labels):\n",
    "            scores[i][j] = score(doc, labels_neighborhoods[label], sim, ngrams, normalize)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.120803, 0.400857, 0.212052],\n",
       "       [0.474738, 0.220495, 0.548871],\n",
       "       [0.116798, 0.432931, 0.310222],\n",
       "       [0.357784, 0.188647, 0.614708]])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_labels = [ 'gun', 'space', 'war;gun' ]\n",
    "p_matrix = predict_dataset(data, sorted_labels, lns2, 'harmonized', ngrams=ngrams, normalize='inter_len')\n",
    "p_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['space', 'war;gun', 'space', 'war;gun']"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = np.argmax(p_matrix, axis=1)\n",
    "predicted_labels = [sorted_labels[p] for p in preds]\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefetching the pickle for label war ..\n",
      "Prefetching the pickle for label gun ..\n"
     ]
    }
   ],
   "source": [
    "lns2['war;gun'] = get_label_neighborhood('war;gun', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       space       1.00      1.00      1.00         2\n",
      "     war;gun       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         4\n",
      "   macro avg       1.00      1.00      1.00         4\n",
      "weighted avg       1.00      1.00      1.00         4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(predicted_labels, [['space', 'gun'], ['war;gun', 'gun'], ['space'], ['war;gun', 'gun']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
